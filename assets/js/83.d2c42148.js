(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{542:function(t,a,e){"use strict";e.r(a);var r=e(14),s=Object(r.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"flink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink"}},[t._v("#")]),t._v(" Flink")]),t._v(" "),e("p"),e("div",{staticClass:"table-of-contents"},[e("ul",[e("li",[e("a",{attrs:{href:"#flink-容错机制"}},[t._v("Flink 容错机制")]),e("ul",[e("li",[e("a",{attrs:{href:"#介绍"}},[t._v("介绍")])]),e("li",[e("a",{attrs:{href:"#checkpointing"}},[t._v("Checkpointing")])]),e("li",[e("a",{attrs:{href:"#copyonwritestatetable"}},[t._v("CopyOnWriteStateTable")])]),e("li",[e("a",{attrs:{href:"#barriers"}},[t._v("Barriers")])]),e("li",[e("a",{attrs:{href:"#state"}},[t._v("State")]),e("ul",[e("li",[e("a",{attrs:{href:"#扩缩容"}},[t._v("扩缩容")])]),e("li",[e("a",{attrs:{href:"#参考博客"}},[t._v("参考博客")])])])]),e("li",[e("a",{attrs:{href:"#exactly-once-vs-at-least-once"}},[t._v("Exactly Once vs. At Least Once")])]),e("li",[e("a",{attrs:{href:"#asynchronous-state-snapshots"}},[t._v("Asynchronous State Snapshots")])]),e("li",[e("a",{attrs:{href:"#recovery"}},[t._v("Recovery")])]),e("li",[e("a",{attrs:{href:"#可查询状态"}},[t._v("可查询状态")])])])]),e("li",[e("a",{attrs:{href:"#定时器-timer"}},[t._v("定时器（Timer）")])]),e("li",[e("a",{attrs:{href:"#streamingfilesink-和-bucketingsink"}},[t._v("StreamingFileSink 和 BucketingSink")]),e("ul",[e("li",[e("a",{attrs:{href:"#参考"}},[t._v("参考")])])])]),e("li",[e("a",{attrs:{href:"#flink-hive-集成"}},[t._v("Flink Hive 集成")]),e("ul",[e("li",[e("a",{attrs:{href:"#参考"}},[t._v("参考")])]),e("li",[e("a",{attrs:{href:"#hivecatalog"}},[t._v("HiveCatalog")]),e("ul",[e("li",[e("a",{attrs:{href:"#两种类型的表"}},[t._v("两种类型的表")])])])]),e("li",[e("a",{attrs:{href:"#hive方言"}},[t._v("Hive方言")]),e("ul",[e("li",[e("a",{attrs:{href:"#两种方言"}},[t._v("两种方言")])]),e("li",[e("a",{attrs:{href:"#_2-设置方式"}},[t._v("2. 设置方式")])])])])])]),e("li",[e("a",{attrs:{href:"#flink启动与停止"}},[t._v("Flink启动与停止")]),e("ul",[e("li",[e("a",{attrs:{href:"#flink流式人物何时停止"}},[t._v("Flink流式人物何时停止?")])])])]),e("li",[e("a",{attrs:{href:"#flink应用"}},[t._v("Flink应用")]),e("ul",[e("li",[e("a",{attrs:{href:"#数据同步"}},[t._v("数据同步")])]),e("li",[e("a",{attrs:{href:"#数据去重"}},[t._v("数据去重")])])])]),e("li",[e("a",{attrs:{href:"#rocketsdb"}},[t._v("RocketsDB")]),e("ul",[e("li",[e("a",{attrs:{href:"#lsm-tree"}},[t._v("LSM-Tree")])])])])])]),e("p"),t._v(" "),e("h2",{attrs:{id:"flink-容错机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink-容错机制"}},[t._v("#")]),t._v(" Flink 容错机制")]),t._v(" "),e("h3",{attrs:{id:"介绍"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#介绍"}},[t._v("#")]),t._v(" 介绍")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://segmentfault.com/a/1190000008129552",target:"_blank",rel:"noopener noreferrer"}},[t._v("【译】Apache Flink 容错机制 - 苏州谷歌开发者社区 - SegmentFault 思否"),e("OutboundLink")],1)]),t._v(" "),e("ul",[e("li",[t._v("Apache Flink 提供了可以恢复数据流应用到一致状态的容错机制")]),t._v(" "),e("li",[t._v("发生故障时，程序的每条记录只会作用于状态一次（exactly-once）")]),t._v(" "),e("li",[t._v("容错机制通过持续"),e("strong",[t._v("创建分布式数据流")]),t._v("的快照来实现\n"),e("ul",[e("li",[t._v("轻量")]),t._v(" "),e("li",[t._v("可配置")]),t._v(" "),e("li",[t._v("性能影响小")])])]),t._v(" "),e("li",[t._v("为了容错机制生效，数据源（例如 queue 或者 broker）需要能重放数据流")])]),t._v(" "),e("h3",{attrs:{id:"checkpointing"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#checkpointing"}},[t._v("#")]),t._v(" Checkpointing")]),t._v(" "),e("ul",[e("li",[t._v("Flink 容错机制的核心就是持续创建分布式数据流及其状态的一致快照")]),t._v(" "),e("li",[t._v("遇到故障，充当可以回退的一致性检查点（checkpoint）")]),t._v(" "),e("li",[t._v("受分布式快照算法 "),e("a",{attrs:{href:"http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Chandy-Lamport"),e("OutboundLink")],1),t._v(" 启发")])]),t._v(" "),e("h3",{attrs:{id:"copyonwritestatetable"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#copyonwritestatetable"}},[t._v("#")]),t._v(" CopyOnWriteStateTable")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://blog.csdn.net/u013939918/article/details/106755128",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flink 中的 CopyOnWriteStateTable_u013939918的博客-CSDN博客_copyonwritestatemap"),e("OutboundLink")],1)])]),t._v(" "),e("h3",{attrs:{id:"barriers"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#barriers"}},[t._v("#")]),t._v(" Barriers")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("快照核心概念- 数据栅栏")])]),t._v(" "),e("li",[e("p",[t._v("插入到数据流中，同数据流动")])]),t._v(" "),e("li",[e("p",[t._v("Barrier 分割数据流， 前面一部分进入到当前快照，另一部分进入到下一次，每个Barrier有快照ID，并且之前的数据进入了此快照。")]),t._v(" "),e("ul",[e("li",[e("img",{attrs:{src:"https://static.lovedata.net/20-05-20-693ead1dcfb4534524efba10634defe2.png",alt:"image"}})]),t._v(" "),e("li",[e("img",{attrs:{src:"https://static.lovedata.net/20-05-20-d237d343ea98ca60b7dbb6ef08010ed0.png",alt:"image"}})])])])]),t._v(" "),e("h3",{attrs:{id:"state"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#state"}},[t._v("#")]),t._v(" State")]),t._v(" "),e("h4",{attrs:{id:"扩缩容"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#扩缩容"}},[t._v("#")]),t._v(" 扩缩容")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://blog.csdn.net/nazeniwaresakini/article/details/104220138",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flink状态的缩放（rescale）与键组（Key Group）设计_LittleMagic's Blog-CSDN博客_flink keygroup"),e("OutboundLink")],1)]),t._v(" "),e("h4",{attrs:{id:"参考博客"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考博客"}},[t._v("#")]),t._v(" 参考博客")]),t._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://juejin.im/post/6844903795483213832",target:"_blank",rel:"noopener noreferrer"}},[t._v("Apache-Flink深度解析-State - 掘金"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("Apache Flink的DAG图中只有边相连的节点有网络通信 也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信")]),t._v(" "),e("li",[t._v("这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘")]),t._v(" "),e("li",[t._v("扩容\n"),e("ol",[e("li",[e("img",{attrs:{src:"https://static.lovedata.net/20-08-03-f0c6ec7fced957e51a2e7c012eeae73e.png",alt:"image"}}),t._v("+")]),t._v(" "),e("li",[t._v("Keystone 扩容\n"),e("ol",[e("li",[t._v("hash(key) mod parallelism(operator)  这种分配方式大多数情况是恢复的state不是本地已有的state 需要一次网络拷贝   "),e("strong",[t._v("OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小")])]),t._v(" "),e("li",[t._v("在Apache Flink中采用的是Key-Groups方式进行分配。")]),t._v(" "),e("li",[t._v("什么决定Key-Groups的个数\n"),e("ol",[e("li",[t._v("key-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group")]),t._v(" "),e("li",[t._v("如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中")])])]),t._v(" "),e("li",[e("img",{attrs:{src:"https://static.lovedata.net/20-08-03-23fbaea0080a3f8b58809a113d58a7fb.png",alt:"image"}})])])])])])]),t._v(" "),e("ul",[e("li",[e("p",[t._v("形式")]),t._v(" "),e("ul",[e("li",[t._v("用户自定义状态")]),t._v(" "),e("li",[t._v("系统状态，缓存数据，windows buffer")])])]),t._v(" "),e("li",[e("p",[t._v("存储")]),t._v(" "),e("ul",[e("li",[t._v("默认存储在JobManager内存之中")]),t._v(" "),e("li",[t._v("生产一部分配置在可靠的分布式存储系统（HDFS）")])])]),t._v(" "),e("li",[e("p",[t._v("包含")]),t._v(" "),e("ul",[e("li",[t._v("对于并行输入数据源：快照创建时数据流中的位置偏移")]),t._v(" "),e("li",[t._v("对于 operator：存储在快照中的状态指针")]),t._v(" "),e("li",[e("img",{attrs:{src:"https://static.lovedata.net/20-05-20-5ea3a0ab93d435c36098b80bdb89471f.png",alt:"image"}})])])])]),t._v(" "),e("h3",{attrs:{id:"exactly-once-vs-at-least-once"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#exactly-once-vs-at-least-once"}},[t._v("#")]),t._v(" Exactly Once vs. At Least Once")]),t._v(" "),e("ul",[e("li",[t._v("对齐操作可能会对流程序增加延迟")]),t._v(" "),e("li",[t._v("Flink 提供了在 checkpoint 时关闭对齐的方法。当 operator 接收到一个 barrier 时，就会打一个快照，而不会等待其他 barrier，会继续处理数据，而当异常恢复的时候，就会有数据被重复输入，也就是At least once")]),t._v(" "),e("li",[t._v("对齐操作只会发生在拥有多输入运算（join)或者多个输出的 operator（重分区、分流）的场景下. Map filter严格仅一次")])]),t._v(" "),e("h3",{attrs:{id:"asynchronous-state-snapshots"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#asynchronous-state-snapshots"}},[t._v("#")]),t._v(" Asynchronous State Snapshots")]),t._v(" "),e("ul",[e("li",[t._v("存储快照的时候，operator继续处理数据")]),t._v(" "),e("li",[t._v("使用rocksdb使用写时复制（copy on write） 类型数据结构")])]),t._v(" "),e("h3",{attrs:{id:"recovery"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#recovery"}},[t._v("#")]),t._v(" Recovery")]),t._v(" "),e("p",[t._v("一旦遇到故障，Flink 选择最近一个完成的 checkpoint k。系统重新部署整个分布式数据流，重置所有 operator 的状态到 checkpoint k。数据源被置为从 Sk 位置读取")]),t._v(" "),e("h3",{attrs:{id:"可查询状态"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#可查询状态"}},[t._v("#")]),t._v(" 可查询状态")]),t._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://blog.csdn.net/wangpei1949/article/details/100608828",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flink DataStream 可查询状态(Queryable State)_王佩的CSDN博客-CSDN博客_flink queryable state"),e("OutboundLink")],1),t._v(" "),e("ol",[e("li",[e("img",{attrs:{src:"https://static.lovedata.net/20-07-31-00e0bd917ec48c7797174dcab02612b5.png",alt:"image"}}),e("img",{attrs:{src:"https://static.lovedata.net/20-07-31-00e0bd917ec48c7797174dcab02612b5.png",alt:"image"}})])])])]),t._v(" "),e("h2",{attrs:{id:"定时器-timer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#定时器-timer"}},[t._v("#")]),t._v(" 定时器（Timer）")]),t._v(" "),e("p",[e("code",[t._v("TimerService")]),t._v(" 在内部维护两种类型的定时器（处理时间和事件时间定时器）并排队执行。")]),t._v(" "),e("p",[e("code",[t._v("TimerService")]),t._v(" 会删除每个键和时间戳重复的定时器，即每个键在每个时间戳上最多有一个定时器。如果为同一时间戳注册了多个定时器，则只会调用一次 onTimer（） 方法。")]),t._v(" "),e("blockquote",[e("p",[t._v("Flink "),e("strong",[t._v("同步调用 onTimer() 和 processElement()")]),t._v(" 方法。因此，不必担心状态的并发修改。")])]),t._v(" "),e("p",[e("a",{attrs:{href:"https://juejin.im/post/6844904057220366349",target:"_blank",rel:"noopener noreferrer"}},[t._v("ProcessFunction：Flink最底层API使用教程 - 掘金"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"streamingfilesink-和-bucketingsink"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#streamingfilesink-和-bucketingsink"}},[t._v("#")]),t._v(" StreamingFileSink 和 BucketingSink")]),t._v(" "),e("h4",{attrs:{id:"参考"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[t._v("#")]),t._v(" 参考")]),t._v(" "),e("ol",[e("li",[e("p",[e("a",{attrs:{href:"https://blog.csdn.net/u013220482/article/details/100901471",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flink1.9系列-StreamingFileSink vs BucketingSink篇_枫叶的落寞的博客-CSDN博客_streamingfilesink"),e("OutboundLink")],1)]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("StreamingFileSink在写hdfs时候，要求hadoop版本必须大于2.7")]),t._v("，但是目前市面开源的稳定版本包含cloudera cdh在内，都是支持hadoop2.6，")]),t._v(" "),e("li",[t._v("所以如果你使用hadoop版本<2.7,那建议你还是使用BucketingSink")])])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://blog.csdn.net/su20145104009/article/details/103311853",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flink实战之StreamingFileSink如何写数据到其它HA的Hadoop集群_苏苏爱自由-CSDN博客_streamingfilesink flink"),e("OutboundLink")],1)])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://blog.csdn.net/kisimple/article/details/83998238",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flink HDFS Sink 如何保证 exactly-once 语义_Just for Fun LA-CSDN博客_flink hdfs sink"),e("OutboundLink")],1)])])]),t._v(" "),e("h2",{attrs:{id:"flink-hive-集成"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink-hive-集成"}},[t._v("#")]),t._v(" Flink Hive 集成")]),t._v(" "),e("h3",{attrs:{id:"参考-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考-2"}},[t._v("#")]),t._v(" 参考")]),t._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://developer.aliyun.com/article/770294",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flink x Zeppelin ，Hive Streaming 实战解析-阿里云开发者社区"),e("OutboundLink")],1),t._v(" "),e("ol",[e("li",[t._v("Hive Streaming 的意义\n"),e("ol",[e("li",[t._v("Lambda架构 流批分离。离线和实时各自独一份")]),t._v(" "),e("li",[t._v("数据口径问题")]),t._v(" "),e("li",[t._v("离线计算产出延迟大")]),t._v(" "),e("li",[t._v("数据冗余存储")]),t._v(" "),e("li",[t._v("Kappa架构，完全使用实时计算产出数据，历史数据通过回溯消息的消费位点计算，同样也有很多的问题，毕竟没有一劳永逸的架构。")]),t._v(" "),e("li",[t._v("消息中间件无法保留全部历史数据，同样数据都是行式存储，占用空间大")]),t._v(" "),e("li",[t._v("实时计算历史数据力不从心")]),t._v(" "),e("li",[t._v("无法进行Adhoc的分析")])])])])]),t._v(" "),e("li",[e("a",{attrs:{href:"https://developer.aliyun.com/article/763199",target:"_blank",rel:"noopener noreferrer"}},[t._v("深度解读 Flink 1.11：流批一体 Hive 数仓-阿里云开发者社区"),e("OutboundLink")],1),t._v(" "),e("ol",[e("li",[t._v("传统离线数仓是由 Hive 加上 HDFS 的方案，Hive 数仓有着成熟和稳定的大数据分析能力，结合调度和上下游工具，构建一个完整的数据处理分析平台\n"),e("ol",[e("li",[e("img",{attrs:{src:"https://static.lovedata.net/20-09-16-b3ccb5fd4feff9f83c25b5c53feedef7.png",alt:"image"}})]),t._v(" "),e("li",[t._v("离线数仓\n"),e("ol",[e("li",[t._v("流程\n"),e("ol",[e("li",[t._v("Flume 把数据导入 Hive 数仓")]),t._v(" "),e("li",[t._v("调度工具，调度 ETL 作业进行数据处理")]),t._v(" "),e("li",[t._v("在 Hive 数仓的表上，可以进行灵活的 Ad-hoc 查询")]),t._v(" "),e("li",[t._v("调度工具，调度聚合作业输出到BI层的数据库中")])])]),t._v(" "),e("li",[t._v("问题\n"),e("ol",[e("li",[t._v("导入过程不够灵活，这应该是一个灵活 SQL 流计算的过程")]),t._v(" "),e("li",[t._v("基于调度作业的级联计算，实时性太差")]),t._v(" "),e("li",[t._v("ETL 不能有流式的增量计算")])])])])]),t._v(" "),e("li",[t._v("实时数仓\n"),e("ol",[e("li",[e("strong",[t._v("实时数仓")]),t._v("，实时数仓基于 Kafka + Flink streaming，定义全流程的流计算作业，有着秒级甚至毫秒的实时性。")]),t._v(" "),e("li",[e("strong",[t._v("问题")]),t._v(" ： 是历史数据只有 3-15 天，无法在其上做 Ad-hoc 的查询。如果搭建 Lambda 的离线+实时的架构，维护成本、计算存储成本、一致性保证、重复的开发会带来很大的负担。")])])]),t._v(" "),e("li",[t._v("Hive streaming sink\n"),e("ol",[e("li",[t._v("带来 Flink streaming 的实时/准实时的能力")]),t._v(" "),e("li",[t._v("支持 Filesystem connector 的全部 formats(csv,json,avro,parquet,orc)")]),t._v(" "),e("li",[t._v("支持 Hive table 的所有 formats")]),t._v(" "),e("li",[t._v("继承 Datastream StreamingFileSink 的所有特性：Exactly-once、支持HDFS, S3")]),t._v(" "),e("li",[t._v("Partition commit。\n"),e("ol",[e("li",[e("strong",[t._v("Trigger")]),t._v("  什么时候提交")]),t._v(" "),e("li",[t._v("Policy 提交策略")])])])])]),t._v(" "),e("li",[t._v("Hive streaming source\n"),e("ol",[e("li",[t._v("传统Hive ETL 弊端\n"),e("ol",[e("li",[t._v("实时性不强，往往调度最小是小时级。")]),t._v(" "),e("li",[t._v("流程复杂，组件多，容易出现问题。")])])])])]),t._v(" "),e("li",[t._v("Flink 1.11 为此开发了实时化的 Hive 流读\n"),e("ol",[e("li",[t._v("Partition 表，监控 Partition 的生成，增量读取新的 Partition。")]),t._v(" "),e("li",[t._v("非 Partition 表，监控文件夹内新文件的生成，增量读取新的文件。")])])])])])])])]),t._v(" "),e("h3",{attrs:{id:"hivecatalog"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hivecatalog"}},[t._v("#")]),t._v(" HiveCatalog")]),t._v(" "),e("h4",{attrs:{id:"两种类型的表"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#两种类型的表"}},[t._v("#")]),t._v(" 两种类型的表")]),t._v(" "),e("blockquote",[e("p",[t._v("flink 使用 "),e("em",[t._v("is_generic")]),t._v(" 属性累区分一个表是hive兼容表还是通用表，默认是通用表，如果要创建hive兼容表，则需要将 "),e("em",[t._v("is_generic")]),t._v(" 设置为false")])]),t._v(" "),e("ul",[e("li",[t._v("Hive 兼容表\n"),e("ul",[e("li",[t._v("就存储层中的元数据和数据而言，兼容Hive的表是以兼容Hive的方式存储的表。因此，可以从Hive端查询通过Flink创建的Hive兼容表。")])])]),t._v(" "),e("li",[t._v("通用表\n"),e("ul",[e("li",[t._v("用表特定于Flink。使用HiveCatalog创建通用表时，我们只是使用HMS来保留元数据。虽然这些表格对Hive可见，但Hive不太可能能够理解元数据。因此，在Hive中使用此类表会导致未定义的行为。")])])])]),t._v(" "),e("h3",{attrs:{id:"hive方言"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hive方言"}},[t._v("#")]),t._v(" Hive方言")]),t._v(" "),e("blockquote",[e("p",[t._v("从1.11开始，使用Hive方言的时候，flink允许用户使用hive语法编写sql，通过提供与hive兼容性，改善互操作性，减少"),e("strong",[t._v("切换")])])]),t._v(" "),e("h4",{attrs:{id:"两种方言"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#两种方言"}},[t._v("#")]),t._v(" 两种方言")]),t._v(" "),e("ul",[e("li",[t._v("HIve")]),t._v(" "),e("li",[t._v("default")])]),t._v(" "),e("h4",{attrs:{id:"_2-设置方式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-设置方式"}},[t._v("#")]),t._v(" 2. 设置方式")]),t._v(" "),e("h5",{attrs:{id:"sql-client"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sql-client"}},[t._v("#")]),t._v(" SQL Client")]),t._v(" "),e("ol",[e("li",[e("p",[t._v("yaml文件设置")]),t._v(" "),e("ol",[e("li",[e("div",{staticClass:"language-yaml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-yaml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("execution")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("planner")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" blink\n  "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch\n  "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("result-mode")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" table\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("configuration")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("table.sql-dialect")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" hive\n")])])])])])]),t._v(" "),e("li",[e("p",[t._v("set 属性")]),t._v(" "),e("ol",[e("li",[e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[t._v("Flink SQL"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("set")]),t._v(" table.sql-dialect"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("hive"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" -- to use hive dialect\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("INFO"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" Session property has been set.\n\nFlink SQL"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("set")]),t._v(" table.sql-dialect"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("default"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" -- to use default dialect\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("INFO"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" Session property has been set.\n")])])])])])])]),t._v(" "),e("h5",{attrs:{id:"table-api"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#table-api"}},[t._v("#")]),t._v(" Table API")]),t._v(" "),e("div",{staticClass:"language-java extra-class"},[e("pre",{pre:!0,attrs:{class:"language-java"}},[e("code",[e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("EnvironmentSettings")]),t._v(" settings "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("EnvironmentSettings")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("newInstance")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("useBlinkPlanner")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("build")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TableEnvironment")]),t._v(" tableEnv "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TableEnvironment")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("create")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("settings"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// to use hive dialect")]),t._v("\ntableEnv"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setSqlDialect")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SqlDialect")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("HIVE"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// to use default dialect")]),t._v("\ntableEnv"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setSqlDialect")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SqlDialect")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DEFAULT"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("h2",{attrs:{id:"flink启动与停止"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink启动与停止"}},[t._v("#")]),t._v(" Flink启动与停止")]),t._v(" "),e("h3",{attrs:{id:"flink流式人物何时停止"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink流式人物何时停止"}},[t._v("#")]),t._v(" Flink流式人物何时停止?")]),t._v(" "),e("ol",[e("li",[e("p",[e("a",{attrs:{href:"http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Signal-for-End-of-Stream-td20002.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Apache Flink User Mailing List archive. - Signal for End of Stream"),e("OutboundLink")],1)]),t._v(" "),e("ol",[e("li",[e("p",[t._v("Flink will automatically stop the execution of a DataStream program once all sources have finished to provide data, i.e., when all SourceFunction return from the run() method. The DeserializationSchema.isEndOfStream() method can be used to tell a built-in SourceFunction such as a KafkaConsumer that it should leave the run() method. If you implement your own SourceFunction you can leave run() after you ingested all data. Note, that Flink won't wait for all processing time timers but will immediately shutdown the program after the last in-flight record was processed. Event-time timers will be handled because each source emits a Long.MAX_VALUE watermark after it emitted its last record.")]),t._v(" "),e("p",[t._v("一旦所有源都已完成提供数据，即当所有SourceFunction从run（）方法返回时，Flink将自动停止执行DataStream程序。 DeserializationSchema.isEndOfStream（）方法可用于告知诸如KafkaConsumer之类的内置SourceFunction应该离开run（）方法。 如果实现自己的SourceFunction，则可以在提取所有数据后保留run（）。 请注意，Flink不会等待所有处理时间计时器，但是会在处理了最后一个运行中的记录后立即关闭程序。 将处理事件时间计时器，因为每个源在发出最后一条记录后都会发出Long.MAX_VALUE水印。")])]),t._v(" "),e("li",[e("p",[t._v("所以 run方法运行完，这个流就停止了")])])])])]),t._v(" "),e("h2",{attrs:{id:"flink应用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#flink应用"}},[t._v("#")]),t._v(" Flink应用")]),t._v(" "),e("h3",{attrs:{id:"数据同步"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#数据同步"}},[t._v("#")]),t._v(" 数据同步")]),t._v(" "),e("p",[e("a",{attrs:{href:"http://apache-flink.147419.n8.nabble.com/flink-mysql-cdc-hive-streaming-td8223.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Apache Flink 中文用户邮件列表 - flink mysql cdc + hive streaming疑问"),e("OutboundLink")],1)]),t._v(" "),e("h3",{attrs:{id:"数据去重"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#数据去重"}},[t._v("#")]),t._v(" 数据去重")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.jianshu.com/p/f6042288a6e3",target:"_blank",rel:"noopener noreferrer"}},[t._v("谈谈三种海量数据实时去重方案（w/ Flink） - 简书"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"rocketsdb"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#rocketsdb"}},[t._v("#")]),t._v(" RocketsDB")]),t._v(" "),e("h3",{attrs:{id:"lsm-tree"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#lsm-tree"}},[t._v("#")]),t._v(" LSM-Tree")]),t._v(" "),e("blockquote",[e("p",[e("a",{attrs:{href:"/algorithm/LSM-Tree%E7%AC%94%E8%AE%B0"}},[t._v("LSM-Tree笔记")])])]),t._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://mp.weixin.qq.com/s/2Njngm52jNOo0nu50TvRyQ",target:"_blank",rel:"noopener noreferrer"}},[t._v("彻底搞懂LSM-Tree"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://time.geekbang.org/column/article/225400",target:"_blank",rel:"noopener noreferrer"}},[t._v("24 | RocksDB：不丢数据的高性能KV存储-极客时间"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("https://ranger.uta.edu/~sjiang/pubs/papers/wang14-LSM-SDF.pdf")]),t._v(" "),e("li",[e("a",{attrs:{href:"https://kernelmaker.github.io/lsm-tree",target:"_blank",rel:"noopener noreferrer"}},[t._v("【Paper笔记】The Log structured Merge-Tree（LSM-Tree） · "),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://www.open-open.com/lib/view/open1424916275249.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Log Structured Merge Trees(LSM) 原理 - LSM - 软件开发 - 深度开源"),e("OutboundLink")],1)])])])}),[],!1,null,null,null);a.default=s.exports}}]);